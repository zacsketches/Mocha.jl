{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a feed forward net on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20-Oct 18:28:51:INFO:root:Constructing net imagenet on Mocha.CPUBackend...\n",
      "20-Oct 18:28:51:INFO:root:Topological sorting 11 layers...\n",
      "20-Oct 18:28:51:INFO:root:Setup layers...\n",
      "20-Oct 18:28:51:INFO:root:Network constructed!\n",
      "20-Oct 18:28:51:INFO:root:Loading existing model from snapshots/snapshot-070000.jld\n",
      "20-Oct 18:28:51:DEBUG:root:Loading parameters for layer conv1\n",
      "20-Oct 18:28:51:DEBUG:root:Loading parameters for layer conv2\n",
      "20-Oct 18:28:51:DEBUG:root:Loading parameters for layer conv3\n",
      "20-Oct 18:28:51:DEBUG:root:Loading parameters for layer ip1\n",
      "Loading Tags...\n",
      "     Running iteration 100 ...\n",
      "     Running iteration 200 ...\n",
      "     Running iteration 300 ...\n",
      "     Running iteration 400 ...\n",
      "     Running iteration 500 ...\n",
      "     Running iteration 600 ...\n",
      "     Running iteration 700 ...\n",
      "     Running iteration 800 ...\n",
      "     Running iteration 900 ...\n",
      "     Running iteration 1000 ...\n",
      "     Running iteration 1100 ...\n",
      "     Running iteration 1200 ...\n",
      "     Running iteration 1300 ...\n",
      "     Running iteration 1400 ...\n",
      "     Running iteration 1500 ...\n",
      "     Running iteration 1600 ...\n",
      "     Running iteration 1700 ...\n",
      "     Running iteration 1800 ...\n",
      "     Running iteration 1900 ...\n",
      "     Running iteration 2000 ...\n",
      "0.1755\n",
      "20-Oct 18:29:14:DEBUG:root:Destroying network imagenet\n"
     ]
    }
   ],
   "source": [
    "using Mocha, HDF5, MLBase\n",
    "\n",
    "#1. Load the trained network weights\n",
    "#2. Test the network on a random selection training set\n",
    "\n",
    "backend = CPUBackend()\n",
    "init(backend)\n",
    "\n",
    "image_x = 32\n",
    "image_y = 32\n",
    "image_depth = 3\n",
    "\n",
    "# How many samples to run in the validation test\n",
    "N = 2000\n",
    "\n",
    "#Create a memory data layer in the prediction model that we computation\n",
    "#sequentially tie to the next image we want to predict on.\n",
    "mem_data = MemoryDataLayer(name=\"data\", tops=[:data], batch_size=1,\n",
    "    data=Array[zeros(Float32, image_x, image_y, image_depth, 1)])\n",
    "\n",
    "#Copy over the rest of the layers from the training network\n",
    "conv1_layer = ConvolutionLayer(name=\"conv1\", n_filter=32, kernel=(5,5), pad=(2,2),\n",
    "    stride=(1,1), filter_init=GaussianInitializer(std=0.0001),\n",
    "    bottoms=[:data], tops=[:conv1])\n",
    "pool1_layer = PoolingLayer(name=\"pool1\", kernel=(3,3), stride=(2,2), neuron=Neurons.ReLU(),\n",
    "    bottoms=[:conv1], tops=[:pool1])\n",
    "norm1_layer = LRNLayer(name=\"norm1\", kernel=3, scale=5e-5, power=0.75, mode=LRNMode.WithinChannel(),\n",
    "    bottoms=[:pool1], tops=[:norm1])\n",
    "conv2_layer = ConvolutionLayer(name=\"conv2\", n_filter=32, kernel=(5,5), pad=(2,2),\n",
    "    stride=(1,1), filter_init=GaussianInitializer(std=0.01),\n",
    "    bottoms=[:norm1], tops=[:conv2], neuron=Neurons.ReLU())\n",
    "pool2_layer = PoolingLayer(name=\"pool2\", kernel=(3,3), stride=(2,2), pooling=Pooling.Mean(),\n",
    "    bottoms=[:conv2], tops=[:pool2])\n",
    "norm2_layer = LRNLayer(name=\"norm2\", kernel=3, scale=5e-5, power=0.75, mode=LRNMode.WithinChannel(),\n",
    "    bottoms=[:pool2], tops=[:norm2])\n",
    "conv3_layer = ConvolutionLayer(name=\"conv3\", n_filter=64, kernel=(5,5), pad=(2,2),\n",
    "    stride=(1,1), filter_init=GaussianInitializer(std=0.01),\n",
    "    bottoms=[:norm2], tops=[:conv3], neuron=Neurons.ReLU())\n",
    "pool3_layer = PoolingLayer(name=\"pool3\", kernel=(3,3), stride=(2,2), pooling=Pooling.Mean(),\n",
    "    bottoms=[:conv3], tops=[:pool3])\n",
    "ip1_layer   = InnerProductLayer(name=\"ip1\", output_dim=10, weight_init=GaussianInitializer(std=0.01),\n",
    "    weight_regu=L2Regu(250), bottoms=[:pool3], tops=[:ip1])\n",
    "\n",
    "#Instead of a SoftmaxLossLayer like the training script in this file we\n",
    "#have a pure SoftmaxLayer in order to determine the probability of the \n",
    "#image compared against the different n classification options\n",
    "softmax_layer = SoftmaxLayer(name=\"prob\", tops=[:prob], bottoms=[:ip1])\n",
    "\n",
    "#Build the network\n",
    "common_layers = [conv1_layer, pool1_layer, norm1_layer, conv2_layer, pool2_layer, norm2_layer,\n",
    "                 conv3_layer, pool3_layer, ip1_layer]\n",
    "run_net = Net(\"imagenet\", backend, [mem_data, common_layers..., softmax_layer])\n",
    "\n",
    "#Load the network snapshot from the training data\n",
    "load_snapshot(run_net, \"snapshots/snapshot-070000.jld\")\n",
    "\n",
    "#Now run 1000 training data samples through the network\n",
    "fid = h5open(\"data/train.hdf5\", \"r\")\n",
    "features = read(fid, \"data\")\n",
    "labels = read(fid, \"label\")\n",
    "\n",
    "#Randomize the data\n",
    "rp = randperm(size(features,4))\n",
    "features = features[:,:,:,rp]\n",
    "labels = labels[rp]\n",
    "\n",
    "#Set up some constants used in the loop\n",
    "offset = 1  #my tags vector is 1-indexed but the labels vector is \n",
    "                  #is 0-indexed.  So I have to add an offset of 1 to the label value to \n",
    "                  # match my predictive value \n",
    "println(\"Loading Tags...\")\n",
    "tags = []\n",
    "fname = \"batches.meta.txt\"\n",
    "open(fname) do file\n",
    "    for line in eachline(file)\n",
    "        line = strip(line)\n",
    "        if length(line)>0 \n",
    "            push!(tags, strip(line))\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "#set up a list to capture results\n",
    "gts = []\n",
    "preds = []\n",
    "\n",
    "printed_flag = 0;\n",
    "for j in 1:N\n",
    "    if j%100==0\n",
    "        println(\"     Running iteration $j ...\") \n",
    "    end\n",
    " \n",
    "    #Load the jth image and label \n",
    "    get_layer(run_net, \"data\").data[1][:,:,image_depth,1] = features[:,:,image_depth,j]\n",
    "    gt = Int64(labels[j])+offset\n",
    "    push!(gts, gt)\n",
    "\n",
    "    # Run the net forward and use MLBase to classify the output blob\n",
    "    forward(run_net)\n",
    "    prediction = classify(run_net.output_blobs[:prob].data)\n",
    "    push!(preds, prediction[1])\n",
    "        \n",
    "end\n",
    "\n",
    "gts = convert(Array{Int64}, gts)\n",
    "preds = convert(Array{Int64}, preds)\n",
    "println(correctrate(gts, preds))\n",
    "confusmat(10, gts, preds)\n",
    "\n",
    "close(fid)\n",
    "destroy(run_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a validation set on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20-Oct 18:29:55:INFO:root:Constructing net CIFAR10-test on Mocha.CPUBackend...\n",
      "20-Oct 18:29:55:INFO:root:Topological sorting 11 layers...\n",
      "20-Oct 18:29:55:INFO:root:Setup layers...\n",
      "20-Oct 18:29:56:INFO:root:Network constructed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Mocha.ValidationPerformance(************************************************************\n",
       "          NAME: CIFAR10-test\n",
       "       BACKEND: Mocha.CPUBackend\n",
       "  ARCHITECTURE: 11 layers\n",
       "............................................................\n",
       " *** Mocha.HDF5DataLayer(data-test)\n",
       "    Outputs ---------------------------\n",
       "          data: Blob(32 x 32 x 3 x 100)\n",
       "         label: Blob(1 x 100)\n",
       "............................................................\n",
       " *** Mocha.ConvolutionLayer(conv1)\n",
       "    Inputs ----------------------------\n",
       "          data: Blob(32 x 32 x 3 x 100)\n",
       "    Outputs ---------------------------\n",
       "         conv1: Blob(32 x 32 x 32 x 100)\n",
       "............................................................\n",
       " *** Mocha.PoolingLayer(pool1)\n",
       "    Inputs ----------------------------\n",
       "         conv1: Blob(32 x 32 x 32 x 100)\n",
       "    Outputs ---------------------------\n",
       "         pool1: Blob(16 x 16 x 32 x 100)\n",
       "............................................................\n",
       " *** Mocha.LRNLayer(norm1)\n",
       "    Inputs ----------------------------\n",
       "         pool1: Blob(16 x 16 x 32 x 100)\n",
       "    Outputs ---------------------------\n",
       "         norm1: Blob(16 x 16 x 32 x 100)\n",
       "............................................................\n",
       " *** Mocha.ConvolutionLayer(conv2)\n",
       "    Inputs ----------------------------\n",
       "         norm1: Blob(16 x 16 x 32 x 100)\n",
       "    Outputs ---------------------------\n",
       "         conv2: Blob(16 x 16 x 32 x 100)\n",
       "............................................................\n",
       " *** Mocha.PoolingLayer(pool2)\n",
       "    Inputs ----------------------------\n",
       "         conv2: Blob(16 x 16 x 32 x 100)\n",
       "    Outputs ---------------------------\n",
       "         pool2: Blob(8 x 8 x 32 x 100)\n",
       "............................................................\n",
       " *** Mocha.LRNLayer(norm2)\n",
       "    Inputs ----------------------------\n",
       "         pool2: Blob(8 x 8 x 32 x 100)\n",
       "    Outputs ---------------------------\n",
       "         norm2: Blob(8 x 8 x 32 x 100)\n",
       "............................................................\n",
       " *** Mocha.ConvolutionLayer(conv3)\n",
       "    Inputs ----------------------------\n",
       "         norm2: Blob(8 x 8 x 32 x 100)\n",
       "    Outputs ---------------------------\n",
       "         conv3: Blob(8 x 8 x 64 x 100)\n",
       "............................................................\n",
       " *** Mocha.PoolingLayer(pool3)\n",
       "    Inputs ----------------------------\n",
       "         conv3: Blob(8 x 8 x 64 x 100)\n",
       "    Outputs ---------------------------\n",
       "         pool3: Blob(4 x 4 x 64 x 100)\n",
       "............................................................\n",
       " *** Mocha.InnerProductLayer(ip1)\n",
       "    Inputs ----------------------------\n",
       "         pool3: Blob(4 x 4 x 64 x 100)\n",
       "    Outputs ---------------------------\n",
       "           ip1: Blob(10 x 100)\n",
       "............................................................\n",
       " *** Mocha.AccuracyLayer(accuracy)\n",
       "    Inputs ----------------------------\n",
       "           ip1: Blob(10 x 100)\n",
       "         label: Blob(1 x 100)\n",
       "************************************************************\n",
       ",Function[])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Mocha, HDF5\n",
    "\n",
    "data_tr_layer = HDF5DataLayer(name=\"data-train\", source=\"data/train.txt\", batch_size=100, shuffle=@windows ? false : true)\n",
    "\n",
    "data_tt_layer = HDF5DataLayer(name=\"data-test\", source=\"data/test.txt\", batch_size=100, shuffle=@windows ? false : true)\n",
    "\n",
    "\n",
    "conv1_layer = ConvolutionLayer(name=\"conv1\", n_filter=32, kernel=(5,5), pad=(2,2),\n",
    "    stride=(1,1), filter_init=GaussianInitializer(std=0.0001),\n",
    "    bottoms=[:data], tops=[:conv1])\n",
    "pool1_layer = PoolingLayer(name=\"pool1\", kernel=(3,3), stride=(2,2), neuron=Neurons.ReLU(),\n",
    "    bottoms=[:conv1], tops=[:pool1])\n",
    "norm1_layer = LRNLayer(name=\"norm1\", kernel=3, scale=5e-5, power=0.75, mode=LRNMode.WithinChannel(),\n",
    "    bottoms=[:pool1], tops=[:norm1])\n",
    "conv2_layer = ConvolutionLayer(name=\"conv2\", n_filter=32, kernel=(5,5), pad=(2,2),\n",
    "    stride=(1,1), filter_init=GaussianInitializer(std=0.01),\n",
    "    bottoms=[:norm1], tops=[:conv2], neuron=Neurons.ReLU())\n",
    "pool2_layer = PoolingLayer(name=\"pool2\", kernel=(3,3), stride=(2,2), pooling=Pooling.Mean(),\n",
    "    bottoms=[:conv2], tops=[:pool2])\n",
    "norm2_layer = LRNLayer(name=\"norm2\", kernel=3, scale=5e-5, power=0.75, mode=LRNMode.WithinChannel(),\n",
    "    bottoms=[:pool2], tops=[:norm2])\n",
    "conv3_layer = ConvolutionLayer(name=\"conv3\", n_filter=64, kernel=(5,5), pad=(2,2),\n",
    "    stride=(1,1), filter_init=GaussianInitializer(std=0.01),\n",
    "    bottoms=[:norm2], tops=[:conv3], neuron=Neurons.ReLU())\n",
    "pool3_layer = PoolingLayer(name=\"pool3\", kernel=(3,3), stride=(2,2), pooling=Pooling.Mean(),\n",
    "    bottoms=[:conv3], tops=[:pool3])\n",
    "ip1_layer   = InnerProductLayer(name=\"ip1\", output_dim=10, weight_init=GaussianInitializer(std=0.01),\n",
    "    weight_regu=L2Regu(250), bottoms=[:pool3], tops=[:ip1])\n",
    "\n",
    "loss_layer  = SoftmaxLossLayer(name=\"softmax\", bottoms=[:ip1, :label])\n",
    "acc_layer   = AccuracyLayer(name=\"accuracy\", bottoms=[:ip1, :label])\n",
    "\n",
    "common_layers = [conv1_layer, pool1_layer, norm1_layer, conv2_layer, pool2_layer, norm2_layer,\n",
    "                 conv3_layer, pool3_layer, ip1_layer]\n",
    "\n",
    "backend = DefaultBackend()\n",
    "init(backend)\n",
    "\n",
    "# show performance on test data \n",
    "test_net = Net(\"CIFAR10-test\", backend, [data_tt_layer, common_layers..., acc_layer])\n",
    "ValidationPerformance(test_net)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.4.6",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
